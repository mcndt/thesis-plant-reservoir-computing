{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multirun experiment pipeline 0.2.0 (WheatFspm)\n",
    "\n",
    "The following notebook establishes a generalized pipeline for evaluating a computing reservoir against a given task, given multiple experimental runs of the same reservoir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../'))  # for importing local packages from src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NEMA_H0 = '../datasets/dataset_NEMA_NEMA_H0.csv'\n",
    "DATASET_NEMA_H3 = '../datasets/dataset_NEMA_NEMA_H3.csv'\n",
    "DATASET_NEMA_H15 = '../datasets/dataset_NEMA_NEMA_H15.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets\n",
    "\n",
    "These datasets were collected and converted in the WheatFspm repository.\n",
    "\n",
    "There are three simulations made available in the WheatFspm repository that are useable for RC experiments: NEMA H0, H3 and H15.\n",
    "\n",
    "We can try using these datasets in two different ways:\n",
    "\n",
    "1. Treat every dataset as a separate plant, training a readout for each simulation run.\n",
    "2. Concatenating the three datasets as observed behavior of a single plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.rc_dataset import ExperimentDataset\n",
    "\n",
    "dataset_nema_h0 = ExperimentDataset(csv_path=DATASET_NEMA_H0)\n",
    "dataset_nema_h3 = ExperimentDataset(csv_path=DATASET_NEMA_H3)\n",
    "dataset_nema_h15 = ExperimentDataset(csv_path=DATASET_NEMA_H15)\n",
    "\n",
    "datasets = [\n",
    "  ('NEMA_H0', dataset_nema_h0), \n",
    "  ('NEMA_H3', dataset_nema_h3), \n",
    "  ('NEMA_H15', dataset_nema_h15)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining targets and observed state variables\n",
    "\n",
    "These were selected in a previous notebook, `2022_03_23_wheatfspm_dataset_inspection.ipynb` and are defined in a config file for reuse among notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "\t- input_air_temperature\n",
      "\t- input_humidity\n",
      "\t- input_PARi\n",
      "\t- output__axes__Total_Transpiration\n",
      "\t- output__axes__C_exudated\n",
      "\t- output__axes__SAM_temperature\n",
      "\t- output__axes__delta_teq\n",
      "\t- output__axes__sum_respi_shoot\n",
      "\t- output__organ_roots__N_exudation\n",
      "\n",
      "State variables:\n",
      "\t- state__An\n",
      "\t- state__Transpiration\n",
      "\t- state__S_Sucrose\n",
      "\t- state__Ts\n",
      "\t- state__gs\n",
      "\t- state__Ag\n",
      "\t- state__Tr\n",
      "\t- state__sucrose\n",
      "\t- state__Rd\n",
      "\t- state__sum_respi\n",
      "\t- state__Photosynthesis\n",
      "\t- state__PARa\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "from model_config import targets, state_variables\n",
    "\n",
    "print(f'Targets:')\n",
    "for target in targets:\n",
    "  print(f'\\t- {target}')\n",
    "\n",
    "print(f'\\nState variables:')\n",
    "for state_var in state_variables:\n",
    "  print(f'\\t- {state_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing, grouping and train-test splitting\n",
    "\n",
    "The available datasets will be processed into 4 datasets:\n",
    "\n",
    "- NEMA_H0\n",
    "- NEMA_H3\n",
    "- NEMA_H15\n",
    "- NEMA_COMBINED (concatenated as data from the same plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.learning.preprocessing import generate_mask\n",
    "\n",
    "\n",
    "WARMUP_STEPS = 4 * 24\n",
    "DAY_MASK = generate_mask(5, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "- Readout model is a standard RidgeRegression model with intercept term and CV-tuned regularization strength $\\alpha$.\n",
    "- CV search grid is a progression of logarithmicly spaced values for regularization strength $\\alpha$.\n",
    "- CV and testing metric is NMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from src.learning.scorers import nmse_scorer\n",
    "\n",
    "readout = Pipeline([\n",
    "  ('ridge_regression', Ridge(alpha=1, fit_intercept=True))\n",
    "])\n",
    "\n",
    "search_grid = [{\n",
    "  'ridge_regression__alpha': 10 ** np.linspace(np.log10(1e-4), np.log10(1e2), 50)\n",
    "}]\n",
    "\n",
    "scorer = nmse_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a manifest of all experiments to run\n",
    "\n",
    "Currently we are only benchmarking direct target prediction, but in the future there will be other tasks generated from the base targets as well. These will be generated in this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "from wheatfspm_pipeline_utils import generate_X_y_groups, generate_X_y_groups_baseline, direct_target_generator, direct_reservoir_generator\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "\n",
    "def generate_ModelFitParams_fpsm_reservoir(*, datasets, target_state_pairs, target_generator, state_generator, \n",
    "                                          folds, warmup_steps, day_mask, combined_only=True, note=''):\n",
    "  params_list = []\n",
    "\n",
    "  for target, state_var in target_state_pairs:\n",
    "    preprocessed_data = generate_X_y_groups(\n",
    "      datasets=datasets, \n",
    "      target=target, \n",
    "      state_var=state_var,\n",
    "      target_generator=direct_target_generator, \n",
    "      state_generator=direct_reservoir_generator,\n",
    "      warmup_steps=warmup_steps, \n",
    "      day_mask=day_mask,\n",
    "      combined_only=combined_only,\n",
    "    )\n",
    "\n",
    "    for name, (X, y, groups) in preprocessed_data.items():\n",
    "      params = ModelFitParams(\n",
    "        X_name=state_var,\n",
    "        y_name=target,\n",
    "        dataset_name=name,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        groups=groups,\n",
    "        folds=folds,\n",
    "        note=note\n",
    "      )\n",
    "      params_list.append(params)\n",
    "\n",
    "  return params_list\n",
    "\n",
    "\n",
    "def generate_ModelFitParams_baseline_reservoir(*, datasets, target_env_pairs, target_generator, folds,\n",
    "                                               warmup_steps, day_mask, combined_only=True, note=''):\n",
    "  params_list = []\n",
    "\n",
    "  # for target in targets:\n",
    "  for target, (env_name, env_targets) in target_env_pairs:\n",
    "    preprocessed_data = generate_X_y_groups_baseline(\n",
    "      datasets=datasets,\n",
    "      target=target,\n",
    "      env_targets=env_targets,\n",
    "      prefix=env_name,\n",
    "      target_generator=target_generator,\n",
    "      warmup_steps=warmup_steps,\n",
    "      day_mask=day_mask,\n",
    "      combined_only=combined_only,\n",
    "    )\n",
    "\n",
    "    for name, (X, y, groups) in preprocessed_data.items():\n",
    "      params = ModelFitParams(\n",
    "        X_name=env_name,\n",
    "        y_name=target,\n",
    "        dataset_name=f'{name}_{env_name}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        groups=groups,\n",
    "        folds=folds,\n",
    "        note=note\n",
    "      )\n",
    "      params_list.append(params)\n",
    "\n",
    "  return params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config import baseline_reservoirs\n",
    "\n",
    "\n",
    "target_state_pairs = [(target, state_var) for target in targets for state_var in state_variables]\n",
    "\n",
    "model_fit_params_fspm = generate_ModelFitParams_fpsm_reservoir(\n",
    "  datasets=datasets,\n",
    "  target_state_pairs=target_state_pairs,\n",
    "  target_generator=direct_target_generator,\n",
    "  state_generator=direct_reservoir_generator,\n",
    "  folds=LeaveOneGroupOut(),\n",
    "  warmup_steps=WARMUP_STEPS, \n",
    "  day_mask=DAY_MASK\n",
    ")\n",
    "\n",
    "target_env_pairs = [(target, baseline_res) for target in targets for baseline_res in baseline_reservoirs]\n",
    "\n",
    "model_fit_params_env = generate_ModelFitParams_baseline_reservoir(\n",
    "  datasets=datasets,\n",
    "  target_env_pairs=target_env_pairs,\n",
    "  target_generator=direct_target_generator,\n",
    "  folds=LeaveOneGroupOut(),\n",
    "  warmup_steps=WARMUP_STEPS,\n",
    "  day_mask=DAY_MASK\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting all readout functions\n",
    "\n",
    "Process:\n",
    "- for each fitting strategy:\n",
    "  - For each target:\n",
    "    - For each observed state variable:\n",
    "      - For each dataset:\n",
    "        1. Preprocess the data\n",
    "        2. Fit for each dataset\n",
    "        3. Store the resulting training, cross-validation and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of training to testing samples (in groups of 16 samples belonging to the same day)\n",
    "TRAIN_TEST_RATIO = 1\n",
    "\n",
    "# Amount of consecutive groups that are taken as training or testing groups.\n",
    "# e.g. for BLOCKS=4, the split is 1111 0000 1111 0000 ...\n",
    "BLOCKS = 4\n",
    "\n",
    "MODEL_FIT_PARAMS = model_fit_params_fspm + model_fit_params_env\n",
    "\n",
    "TRAIN_TEST_SPLITTER = lambda X, y, groups: train_test_split_alternating(X, y, groups, ratio=1, blocks=4)\n",
    "\n",
    "RESULTS_FILE = 'results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 126 fits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit_test_model() takes 0 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_45016/2892212280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_loops\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mMODEL_FIT_PARAMS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_test_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_TEST_SPLITTER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit_test_model() takes 0 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from src.learning.training import fit_test_model\n",
    "from wheatfspm_pipeline_utils import train_test_split_alternating\n",
    "\n",
    "\n",
    "total_loops = len(MODEL_FIT_PARAMS)\n",
    "print(f'Performing {total_loops} fits...')\n",
    "\n",
    "models = []\n",
    "results = []\n",
    "\n",
    "with tqdm(total=total_loops) as pbar:\n",
    "    for params in MODEL_FIT_PARAMS:\n",
    "        model, result_dict = fit_test_model(\n",
    "            model=readout, \n",
    "            search_grid=search_grid,\n",
    "            scorer=scorer,\n",
    "            train_test_splitter=TRAIN_TEST_SPLITTER, \n",
    "            params=params\n",
    "        )\n",
    "        models.append(model)\n",
    "        results.append(result_dict)\n",
    "        pbar.update(1)\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    results_df.to_csv(RESULTS_FILE)\n",
    "    print(f'Saved scores to {RESULTS_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4121734fd62df77af0346899b5494e4291ab6203437ffd47de4eeaba662aa73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rc-plants')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
