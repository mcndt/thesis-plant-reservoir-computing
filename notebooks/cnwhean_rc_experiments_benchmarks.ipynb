{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN_Wheat: Nonlinear and memory-bound tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))  # for importing local packages from src\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.rc_dataset import ExperimentDataset\n",
    "\n",
    "\n",
    "DATASET_NEMA_H0 = 'datasets/dataset_NEMA_NEMA_H0.csv'\n",
    "DATASET_NEMA_H3 = 'datasets/dataset_NEMA_NEMA_H3.csv'\n",
    "DATASET_NEMA_H15 = 'datasets/dataset_NEMA_NEMA_H15.csv'\n",
    "\n",
    "dataset_nema_h0 = ExperimentDataset(csv_path=DATASET_NEMA_H0)\n",
    "dataset_nema_h3 = ExperimentDataset(csv_path=DATASET_NEMA_H3)\n",
    "dataset_nema_h15 = ExperimentDataset(csv_path=DATASET_NEMA_H15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Shared model parameters for all experiments:\n",
    "\n",
    "- Ridge regresion model\n",
    "  - Fitted bias term\n",
    "  - Tuned regularization parameter\n",
    "  - LeaveOneGroupOut cross-validation to tune param\n",
    "- Datasets\n",
    "  - NEMA_H0, H3 and H15 combined as one dataset\n",
    "  - Grouped by calendar day of the model inputs\n",
    "- Model preprocessing\n",
    "  - ~~4 days of each run are discarded as warmup days~~ **Transforms must now be applied as a pipeline with the benchmark-specific transforms!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = 'NEMA_combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from src.learning.scorers import nmse_scorer\n",
    "from pipeline_cnwheat import GroupGenerator, TimeGenerator\n",
    "from pipeline_base import TrainTestSplitter, DirectTransform, WarmupTransform\n",
    "\n",
    "\n",
    "readout_model = Pipeline([\n",
    "  ('ridge_regression', Ridge(alpha=1, fit_intercept=True))\n",
    "])\n",
    "\n",
    "model_param_grid = [{\n",
    "  'ridge_regression__alpha': 10 ** np.linspace(np.log10(1e-4), np.log10(1e2), 50)\n",
    "}]\n",
    "\n",
    "datasets = [\n",
    "  dataset_nema_h0, \n",
    "  dataset_nema_h3,\n",
    "  dataset_nema_h15\n",
    "]\n",
    "\n",
    "warmup_days = 4\n",
    "\n",
    "shared_pipeline_params = {\n",
    "  # Data generation\n",
    "  'datasets': datasets,\n",
    "  'groups': GroupGenerator(day_length=24),\n",
    "  'time': TimeGenerator(day_length=24),\n",
    "  \n",
    "  # Model training and validation\n",
    "  'readout_model': readout_model,\n",
    "  'model_param_grid': model_param_grid,\n",
    "  'model_scorer': nmse_scorer,\n",
    "  'folds': LeaveOneGroupOut(),\n",
    "  'train_test_split': TrainTestSplitter(block_size=4, test_ratio=0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import (\n",
    "  RCPipeline, \n",
    "  Rescale, \n",
    "  DaylightMask\n",
    ")\n",
    "\n",
    "from pipeline_cnwheat import (\n",
    "  TargetGenerator, \n",
    "  SingleReservoirGenerator,\n",
    "  MultiReservoirGenerator,\n",
    "  TargetReservoirGenerator,\n",
    "  GroupRescale\n",
    ")\n",
    "\n",
    "from model_config_cnwheat import (\n",
    "  baseline_reservoirs,\n",
    "  heterogeneous_reservoirs, \n",
    "  targets, \n",
    "  state_variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delay line benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import DelayLineTransform, CustomWarmupTransform\n",
    "\n",
    "def delay_line_benchmark(target_gen, target_id, res_gen, res_id, rescale, *, delay_steps: [int]):\n",
    "  for d in delay_steps:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={\n",
    "        'target_id': target_id, \n",
    "        'reservoir_id': res_id, \n",
    "        'dataset_id': DATASET_ID,\n",
    "        'benchmark': 'delay_line',\n",
    "        'delay': d,\n",
    "      },\n",
    "      # Data generation\n",
    "      target=target_gen,\n",
    "      reservoir=res_gen,\n",
    "      # Data transformation\n",
    "      transforms=[\n",
    "        # NOTE: remove warmup before transform, otherwise the \n",
    "        # warmup steps are used as delayed prediction targets.\n",
    "        WarmupTransform(warmup_days=4, day_length=24),\n",
    "        DelayLineTransform(delay_steps=d),\n",
    "      ],\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        rescale\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import PolynomialTargetTransform\n",
    "\n",
    "def polynomial_benchmark(target_gen, target_id, res_gen, res_id, rescale, *, exponents: [int]):\n",
    "  for e in exponents:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={\n",
    "        'target_id': target_id, \n",
    "        'reservoir_id': res_id, \n",
    "        'dataset_id': DATASET_ID,\n",
    "        'benchmark': 'polynomial',\n",
    "        'exponent': e,\n",
    "      },\n",
    "      # Data generation\n",
    "      target=target_gen,\n",
    "      reservoir=res_gen,\n",
    "      # Data transformation\n",
    "      transforms=[\n",
    "        WarmupTransform(warmup_days=4, day_length=24),\n",
    "        PolynomialTargetTransform(poly_coefs=[0] * e + [1]),\n",
    "      ],\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        rescale\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NARMA benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import NarmaTargetTransform\n",
    "\n",
    "def narma_benchmark(target_gen, target_id, res_gen, res_id, rescale, *, n_values: [int]):\n",
    "  for n in n_values:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={\n",
    "        'target_id': target_id, \n",
    "        'reservoir_id': res_id, \n",
    "        'dataset_id': DATASET_ID,\n",
    "        'benchmark': 'NARMA',\n",
    "        'narma_n': n,\n",
    "      },\n",
    "      # Data generation\n",
    "      target=target_gen,\n",
    "      reservoir=res_gen,\n",
    "      # Data transformation\n",
    "      transforms=[\n",
    "        # NOTE: remove warmup before transform, otherwise the \n",
    "        # warmup steps are used as input to the NARMA system.\n",
    "        WarmupTransform(warmup_days=4, day_length=24),  \n",
    "        NarmaTargetTransform(n=n, scale=1),\n",
    "      ],\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        rescale\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Near future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_pred_benchmark(target_gen, target_id, res_gen, res_id, rescale, *, lookahead_steps: [int]):\n",
    "  for l in lookahead_steps:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={\n",
    "        'target_id': target_id, \n",
    "        'reservoir_id': res_id, \n",
    "        'dataset_id': DATASET_ID,\n",
    "        'benchmark': 'near_future_pred',\n",
    "        'lookahead': l,\n",
    "      },\n",
    "      # Data generation\n",
    "      target=target_gen,\n",
    "      reservoir=res_gen,\n",
    "      # Data transformation\n",
    "      transforms=[\n",
    "        # NOTE: remove warmup before transform, otherwise the \n",
    "        # warmup steps are used as delayed prediction targets.\n",
    "        WarmupTransform(warmup_days=4, day_length=24),\n",
    "        DelayLineTransform(delay_steps=-l),\n",
    "      ],\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        rescale\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = TargetGenerator(target=targets[0])\n",
    "test_reservoir = SingleReservoirGenerator(state_var=state_variables[0])\n",
    "rescale = Rescale(per_feature=False)\n",
    "\n",
    "lookahead_steps = [0, 1, 2]\n",
    "\n",
    "test_narma_pipelines = list(future_pred_benchmark(test_target, 'target', test_reservoir, 'res', rescale, lookahead_steps=lookahead_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "benchmarks = [\n",
    "  {\n",
    "    'benchmark': delay_line_benchmark,\n",
    "    'kwargs': {\n",
    "      'delay_steps': np.arange(0, 13)  # 0-12 hours\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    'benchmark': future_pred_benchmark,\n",
    "    'kwargs': {\n",
    "      'lookahead_steps': np.arange(0, 7)  # 0-6 hours\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    'benchmark': polynomial_benchmark,\n",
    "    'kwargs': {\n",
    "      'exponents': np.arange(1, 10)  # exponents 1-9\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    'benchmark': narma_benchmark,\n",
    "    'kwargs': {\n",
    "      'n_values': [2, 5, 10, 20, 50, 100]\n",
    "    }\n",
    "  },\n",
    "]\n",
    "\n",
    "\n",
    "def generate_benchmark_pipelines(target_gen, target_id, reservoir_gen, reservoir_id, rescale):\n",
    "  pipeline_generators = []\n",
    "  for benchmark in benchmarks:\n",
    "    gen = benchmark['benchmark'](target_gen, target_id, reservoir_gen, reservoir_id, rescale, **benchmark['kwargs'])\n",
    "    pipeline_generators.append(gen)\n",
    "  return chain(*pipeline_generators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4725\n"
     ]
    }
   ],
   "source": [
    "target_state_pairs = [(target, state_var) for target in targets for state_var in state_variables]\n",
    "target_env_pairs = [(name, target, env_targets) for target in targets for (name, env_targets) in baseline_reservoirs]\n",
    "target_het_pairs = [(name, target, state_vars) for target in targets for (name, state_vars) in heterogeneous_reservoirs]\n",
    "\n",
    "\n",
    "all_pipelines = []\n",
    "\n",
    "for target, state_var in target_state_pairs:\n",
    "  target_gen = TargetGenerator(target=target)\n",
    "  res_gen = SingleReservoirGenerator(state_var=state_var)\n",
    "  pipelines = generate_benchmark_pipelines(\n",
    "    target_gen, target, res_gen, state_var, \n",
    "    Rescale(per_feature=False)\n",
    "  )\n",
    "  all_pipelines += pipelines\n",
    "\n",
    "for name, target, state_vars in target_het_pairs:\n",
    "  target_gen = TargetGenerator(target=target)\n",
    "  res_gen = MultiReservoirGenerator(state_vars=state_vars)\n",
    "  pipelines = generate_benchmark_pipelines(\n",
    "    target_gen, target, res_gen, name, \n",
    "    GroupRescale(datasets=datasets, state_vars=state_vars)\n",
    "  )\n",
    "  all_pipelines += pipelines\n",
    "\n",
    "for name, target, env_targets in target_env_pairs:\n",
    "  target_gen = TargetGenerator(target=target)\n",
    "  res_gen = TargetReservoirGenerator(targets=env_targets)\n",
    "  pipelines = generate_benchmark_pipelines(\n",
    "    target_gen, target, res_gen, name, \n",
    "    Rescale(per_feature=True)\n",
    "  )\n",
    "  all_pipelines += pipelines\n",
    "\n",
    "\n",
    "print(len(all_pipelines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4725/4725 [30:51<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scores to results_cnwheat_benchmarks.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from pipeline_base import execute_pipeline\n",
    "\n",
    "\n",
    "RESULTS_FILE = 'results_cnwheat_benchmarks.csv'\n",
    "\n",
    "total_loops = len(all_pipelines)\n",
    "results = []\n",
    "\n",
    "\n",
    "with tqdm(total=total_loops) as pbar:\n",
    "  for pipeline in all_pipelines:\n",
    "\n",
    "    try:\n",
    "      result = execute_pipeline(pipeline)\n",
    "      results.append(result)\n",
    "      pbar.update(1)\n",
    "    except Exception as e:\n",
    "      print('An exception occured executing the pipeline with the following metadata:')\n",
    "      print(f'{pipeline.metadata}')\n",
    "      raise e\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df.to_csv(RESULTS_FILE)\n",
    "print(f'Saved scores to {RESULTS_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4121734fd62df77af0346899b5494e4291ab6203437ffd47de4eeaba662aa73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rc-plants')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
