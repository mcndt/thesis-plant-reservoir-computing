{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../'))  # for importing local packages from src\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))  # for importing model config\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath} \\usepackage{gensymb} \\usepackage{siunitx}'\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.rc_dataset import ExperimentDataset\n",
    "\n",
    "\n",
    "DATASET_NEMA_H0 = '../datasets/dataset_NEMA_NEMA_H0.csv'\n",
    "DATASET_NEMA_H3 = '../datasets/dataset_NEMA_NEMA_H3.csv'\n",
    "DATASET_NEMA_H15 = '../datasets/dataset_NEMA_NEMA_H15.csv'\n",
    "\n",
    "dataset_nema_h0 = ExperimentDataset(csv_path=DATASET_NEMA_H0)\n",
    "dataset_nema_h3 = ExperimentDataset(csv_path=DATASET_NEMA_H3)\n",
    "dataset_nema_h15 = ExperimentDataset(csv_path=DATASET_NEMA_H15)\n",
    "\n",
    "datasets = [\n",
    "  dataset_nema_h0, \n",
    "  dataset_nema_h3,\n",
    "  dataset_nema_h15\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from src.learning.scorers import nmse_scorer\n",
    "from pipeline_cnwheat import GroupGenerator, TimeGenerator\n",
    "from pipeline_base import TrainTestSplitter, DirectTransform, WarmupTransform\n",
    "\n",
    "# constants\n",
    "DATASET_ID = 'NEMA_combined'\n",
    "STATE_SIZE = 7\n",
    "N_STATE_SAMPLES = 16\n",
    "state_ids = None\n",
    "\n",
    "# Reservoir generation\n",
    "np.random.seed(42)\n",
    "# NOTE: all measurable reservoirs considered have the same reservoir size of 10.\n",
    "generate_state_sample = lambda : np.random.choice(10, size=STATE_SIZE, replace=False)\n",
    "\n",
    "\n",
    "# Readout model\n",
    "readout_model = Pipeline([\n",
    "  ('ridge_regression', Ridge(alpha=1, fit_intercept=True))\n",
    "])\n",
    "model_param_grid = [{\n",
    "  'ridge_regression__alpha': 10 ** np.linspace(np.log10(1e-4), np.log10(1e2), 50)\n",
    "}]\n",
    "\n",
    "# Regression task pipeline\n",
    "shared_pipeline_params = {\n",
    "  # Data generation\n",
    "  'datasets': datasets,\n",
    "  'groups': GroupGenerator(day_length=24),\n",
    "  'time': TimeGenerator(day_length=24),\n",
    "  \n",
    "  # Model training and validation\n",
    "  'readout_model': readout_model,\n",
    "  'model_param_grid': model_param_grid,\n",
    "  'model_scorer': nmse_scorer,\n",
    "  'folds': LeaveOneGroupOut(),\n",
    "  'train_test_split': TrainTestSplitter(block_size=4, test_ratio=0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from src.learning.scorers import nmse_scorer\n",
    "from pipeline_cnwheat import GroupGenerator, TimeGenerator\n",
    "from pipeline_base import TrainTestSplitter, DirectTransform, WarmupTransform\n",
    "\n",
    "# constants\n",
    "DATASET_ID = 'NEMA_combined'\n",
    "STATE_SIZE = 7\n",
    "\n",
    "# Reservoir generation\n",
    "np.random.seed(42)\n",
    "state_ids = np.random.choice(10, size=STATE_SIZE, replace=False)\n",
    "\n",
    "# Readout model\n",
    "readout_model = Pipeline([\n",
    "  ('ridge_regression', Ridge(alpha=1, fit_intercept=True))\n",
    "])\n",
    "model_param_grid = [{\n",
    "  'ridge_regression__alpha': 10 ** np.linspace(np.log10(1e-4), np.log10(1e2), 50)\n",
    "}]\n",
    "\n",
    "# Regression task pipeline\n",
    "shared_pipeline_params = {\n",
    "  # Data generation\n",
    "  'datasets': datasets,\n",
    "  'groups': GroupGenerator(day_length=24),\n",
    "  'time': TimeGenerator(day_length=24),\n",
    "  \n",
    "  # Model training and validation\n",
    "  'readout_model': readout_model,\n",
    "  'model_param_grid': model_param_grid,\n",
    "  'model_scorer': nmse_scorer,\n",
    "  'folds': LeaveOneGroupOut(),\n",
    "  'train_test_split': TrainTestSplitter(block_size=4, test_ratio=0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import (\n",
    "  RCPipeline, \n",
    "  Rescale, \n",
    "  DaylightMask\n",
    ")\n",
    "\n",
    "from pipeline_cnwheat import (\n",
    "  TargetGenerator, \n",
    "  SingleReservoirGenerator,\n",
    "  MultiReservoirGenerator,\n",
    "  TargetReservoirGenerator,\n",
    "  GroupRescale\n",
    ")\n",
    "\n",
    "from model_config_cnwheat import (\n",
    "  baseline_reservoirs,\n",
    "  heterogeneous_reservoirs, \n",
    "  final_targets, \n",
    "  measurable_reservoirs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_PARi', 'output__custom__PARa']\n",
      "['state__Ts', 'state__Tr', 'state__gs']\n",
      "[('env_all', ['input_air_temperature', 'input_humidity', 'input_PARi']), ('env_temp', ['input_air_temperature']), ('env_humidity', ['input_humidity']), ('env_PAR', ['input_PARi'])]\n",
      "[('state__het_all', ('state__An', 'state__Transpiration', 'state__Ts', 'state__gs', 'state__Ag', 'state__Tr', 'state__Rd', 'state__sum_respi', 'state__PARa'))]\n"
     ]
    }
   ],
   "source": [
    "measurable_reservoirs = ['state__Ts', 'state__Tr', 'state__gs']\n",
    "final_targets = ['input_PARi', 'output__custom__PARa']\n",
    "narma_targets = ['input_PARi']\n",
    "\n",
    "target_state_pairs = [(target, state_var) for target in final_targets for state_var in measurable_reservoirs]\n",
    "narma_pairs = [(target, state_var) for target in narma_targets for state_var in measurable_reservoirs]\n",
    "target_env_pairs = []\n",
    "target_het_pairs = []\n",
    "# target_env_pairs = [(name, target, env_targets) for target in final_targets for (name, env_targets) in baseline_reservoirs]\n",
    "# target_het_pairs = [(name, target, state_vars) for target in final_targets for (name, state_vars) in heterogeneous_reservoirs]\n",
    "\n",
    "print(final_targets)\n",
    "print(measurable_reservoirs)\n",
    "print(baseline_reservoirs)\n",
    "print(heterogeneous_reservoirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import NarmaTargetTransform\n",
    "\n",
    "input_phys_transforms = [\n",
    "  WarmupTransform(warmup_days=4, day_length=24),\n",
    "  DirectTransform()\n",
    "]\n",
    "\n",
    "\n",
    "def generate_single(target_state_pairs, state_ids):\n",
    "  for target, state_var in target_state_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': state_var, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target),\n",
    "      reservoir=SingleReservoirGenerator(state_var=state_var, state_ids=state_ids),\n",
    "      # Data transformation\n",
    "      transforms=input_phys_transforms,\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=False)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def generate_multi(target_het_pairs, state_ids):\n",
    "  for name, target, state_vars in target_het_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': name, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target),\n",
    "      reservoir=MultiReservoirGenerator(state_vars=state_vars, state_ids=state_ids),\n",
    "      # Data transformation\n",
    "      transforms=input_phys_transforms,\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        GroupRescale(datasets=datasets, state_vars=state_vars)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def generate_env(target_env_pairs):\n",
    "  for name, target, env_targets in target_env_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': name, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target),\n",
    "      reservoir=TargetReservoirGenerator(targets=env_targets),\n",
    "       # Data transformation\n",
    "      transforms=input_phys_transforms,\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=True)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def narma_benchmark(target_state_pairs, state_ids, *, n):\n",
    "  for target, state_var in target_state_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={\n",
    "        'target_id': f'{target}_NARMA_{n}', \n",
    "        'reservoir_id': state_var, \n",
    "        'dataset_id': DATASET_ID,\n",
    "        'benchmark': 'NARMA',\n",
    "        'narma_n': n,\n",
    "      },\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target),\n",
    "      reservoir=SingleReservoirGenerator(state_var=state_var, state_ids=state_ids),\n",
    "      # Data transformation\n",
    "      transforms=[\n",
    "        # NOTE: remove warmup before transform, otherwise the \n",
    "        # warmup steps are used as input to the NARMA system.\n",
    "        WarmupTransform(warmup_days=4, day_length=24),  \n",
    "        NarmaTargetTransform(n=n, scale=1),\n",
    "      ],\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=False)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pipelines: 9\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "def generate_pipelines(state_ids):\n",
    "  pipelines = list(chain(\n",
    "    generate_single(target_state_pairs, state_ids),\n",
    "    generate_multi(target_het_pairs, state_ids),\n",
    "    generate_env(target_env_pairs),\n",
    "    narma_benchmark(narma_pairs, state_ids, n=8)\n",
    "  ))\n",
    "  return pipelines\n",
    "\n",
    "\n",
    "pipelines = generate_pipelines(state_ids)\n",
    "print(f'Total pipelines: {len(pipelines)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from pipeline_base import execute_pipeline\n",
    "\n",
    "\n",
    "\n",
    "total_loops = len(pipelines)\n",
    "results = []\n",
    "model_data = []\n",
    "\n",
    "with tqdm(total=total_loops) as pbar:\n",
    "  for pipeline in pipelines:\n",
    "\n",
    "    try:\n",
    "      result, md = execute_pipeline(pipeline, return_model_data=True)\n",
    "      results.append(result)\n",
    "      model_data.append(md)\n",
    "      pbar.update(1)\n",
    "    except Exception as e:\n",
    "      print('An exception occured executing the pipeline with the following metadata:')\n",
    "      print(f'{pipeline.metadata}')\n",
    "      raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_PARi__true',\n",
       " 'input_PARi__state__Ts',\n",
       " 'input_PARi__state__Tr',\n",
       " 'input_PARi__state__gs',\n",
       " 'output__custom__PARa__true',\n",
       " 'output__custom__PARa__state__Ts',\n",
       " 'output__custom__PARa__state__Tr',\n",
       " 'output__custom__PARa__state__gs',\n",
       " 'input_PARi_NARMA_8__true',\n",
       " 'input_PARi_NARMA_8__state__Ts',\n",
       " 'input_PARi_NARMA_8__state__Tr',\n",
       " 'input_PARi_NARMA_8__state__gs']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnwheat_pred = {}\n",
    "\n",
    "\n",
    "for result, md in zip(results, model_data):\n",
    "  model = md['final_model']\n",
    "  X_test, y_test = md['test_data']\n",
    "  target_id = result['target_id']\n",
    "  reservoir_id = result['reservoir_id']\n",
    "  y_pred = model.predict(X_test)\n",
    "  cnwheat_pred[f'{target_id}__true'] = y_test\n",
    "  cnwheat_pred[f'{target_id}__{reservoir_id}'] = y_pred\n",
    "\n",
    "\n",
    "list(cnwheat_pred.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_pred_fixed = {}\n",
    "\n",
    "for name, pred in cnwheat_pred.items():\n",
    "  l = len(pred)\n",
    "  new_pred = np.ones(736) * np.NaN\n",
    "  new_pred[:l] = pred\n",
    "  cn_pred_fixed[name] = new_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(cn_pred_fixed)\n",
    "pred_df.to_csv('results_cnwheat_input_phys_predictions.csv')\n",
    "\n",
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4121734fd62df77af0346899b5494e4291ab6203437ffd47de4eeaba662aa73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rc-plants')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
