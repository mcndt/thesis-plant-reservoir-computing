{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../'))  # for importing local packages from src\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))  # for importing model config\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.rc_dataset import ExperimentDataset\n",
    "\n",
    "DATASET_PATH = '../datasets/hydroshoot_large_trimmed.csv'\n",
    "\n",
    "dataset = ExperimentDataset(csv_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from src.learning.scorers import nmse_scorer\n",
    "from pipeline_hydroshoot import GroupGenerator, TimeGenerator\n",
    "from pipeline_base import TrainTestSplitter, DirectTransform, WarmupTransform\n",
    "\n",
    "# constants\n",
    "DATASET_ID = 'HydroShoot_large'\n",
    "STATE_SIZE = 32\n",
    "N_STATE_SAMPLES = 16\n",
    "state_ids = None\n",
    "\n",
    "\n",
    "# Reservoir generation\n",
    "np.random.seed(42)\n",
    "generate_state_sample = lambda : np.random.choice(dataset.state_size(), size=STATE_SIZE, replace=False)\n",
    "\n",
    "\n",
    "# readout model\n",
    "readout_model = Pipeline([\n",
    "  ('ridge_regression', Ridge(alpha=1, fit_intercept=True))\n",
    "])\n",
    "model_param_grid = [{\n",
    "  'ridge_regression__alpha': 10 ** np.linspace(np.log10(1e-4), np.log10(1e2), 50)\n",
    "}]\n",
    "\n",
    "\n",
    "# Regression task pipeline\n",
    "datasets = (dataset,)\n",
    "run_ids = dataset.get_run_ids()\n",
    "shared_pipeline_params = {\n",
    "  # Data generation\n",
    "  'datasets': datasets,\n",
    "  'groups': GroupGenerator(day_length=24, run_ids=run_ids, days_between_runs=1),\n",
    "  'time': TimeGenerator(day_length=24, run_ids=run_ids),\n",
    "  \n",
    "  # Model training and validation\n",
    "  'readout_model': readout_model,\n",
    "  'model_param_grid': model_param_grid,\n",
    "  'model_scorer': nmse_scorer,\n",
    "  'folds': GroupKFold(n_splits=5),\n",
    "  'train_test_split': TrainTestSplitter(block_size=4, test_ratio=0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import (\n",
    "  RCPipeline, \n",
    "  Rescale, \n",
    "  DaylightMask\n",
    ")\n",
    "\n",
    "from pipeline_hydroshoot import (\n",
    "  TargetGenerator, \n",
    "  SingleReservoirGenerator,\n",
    "  MultiReservoirGenerator,\n",
    "  TargetReservoirGenerator,\n",
    "  GroupRescale\n",
    ")\n",
    "\n",
    "\n",
    "from model_config_hydroshoot import (\n",
    "  targets, \n",
    "  measurable_reservoirs,\n",
    "  baseline_reservoirs,\n",
    "  heterogeneous_reservoirs, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Physiological Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_Tac', 'input_u', 'input_hs', 'input_Rg', 'output_Rg', 'output_An', 'output_E', 'output_Tleaf']\n",
      "['state_An', 'state_Tlc', 'state_gs', 'state_E', 'state_Flux', 'state_psi_head']\n",
      "[('env_all', ['input_Tac', 'input_u', 'input_hs', 'input_Rg']), ('env_temp', ['input_Tac']), ('env_humidity', ['input_hs']), ('env_PAR', ['input_Rg']), ('env_wind', ['input_u'])]\n",
      "[('state_all', ('state_An', 'state_Tlc', 'state_gs', 'state_E', 'state_Flux', 'state_psi_head'))]\n"
     ]
    }
   ],
   "source": [
    "target_state_pairs = [(target, state_var) for target in targets for state_var in measurable_reservoirs]\n",
    "target_env_pairs = [(name, target, env_targets) for target in targets for (name, env_targets) in baseline_reservoirs]\n",
    "target_het_pairs = [(name, target, state_vars) for target in targets for (name, state_vars) in heterogeneous_reservoirs]\n",
    "\n",
    "print(targets)\n",
    "print(measurable_reservoirs)\n",
    "print(baseline_reservoirs)\n",
    "print(heterogeneous_reservoirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_phys_transforms = [\n",
    "  WarmupTransform(warmup_days=4, day_length=24),\n",
    "  DirectTransform()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single(target_state_pairs, state_ids):\n",
    "  for target, state_var in target_state_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': state_var, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target, run_ids=run_ids),\n",
    "      reservoir=SingleReservoirGenerator(state_var=state_var, run_ids=run_ids, state_ids=state_ids),\n",
    "      # Data transformation\n",
    "      transforms=input_phys_transforms,\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=False)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def generate_multi(target_het_pairs, state_ids):\n",
    "  for name, target, state_vars in target_het_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': name, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target, run_ids=run_ids),\n",
    "      reservoir=MultiReservoirGenerator(state_vars=state_vars, run_ids=run_ids, state_ids=state_ids),\n",
    "      # Data transformation\n",
    "      transforms=input_phys_transforms,\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        GroupRescale(datasets=datasets, state_vars=state_vars, state_ids=state_ids)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def generate_env(target_env_pairs):\n",
    "  for name, target, env_targets in target_env_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': name, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target, run_ids=run_ids),\n",
    "      reservoir=TargetReservoirGenerator(targets=env_targets, run_ids=run_ids),\n",
    "      # Data transformation\n",
    "      transforms=input_phys_transforms,\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=True)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pipelines: 96\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "def generate_pipelines(state_ids):\n",
    "  pipelines = list(chain(\n",
    "    generate_single(target_state_pairs, state_ids),\n",
    "    generate_multi(target_het_pairs, state_ids),\n",
    "    generate_env(target_env_pairs)\n",
    "  ))\n",
    "  return pipelines\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "state_ids = generate_state_sample()\n",
    "pipelines = generate_pipelines(state_ids)\n",
    "print(f'Total pipelines: {len(pipelines)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "pipes = generate_pipelines(state_ids)\n",
    "n_pipelines = len(pipes)\n",
    "print(n_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [09:01<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scores to results_hydroshoot_input_phys_16sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from pipeline_base import execute_pipeline\n",
    "\n",
    "\n",
    "RESULTS_FILE = 'results_hydroshoot_input_phys_16sample.csv'\n",
    "\n",
    "np.random.seed(42)\n",
    "all_state_samples = [generate_state_sample() for _ in range(N_STATE_SAMPLES)]\n",
    "\n",
    "total_loops = n_pipelines * N_STATE_SAMPLES\n",
    "results = []\n",
    "\n",
    "\n",
    "with tqdm(total=total_loops) as pbar:\n",
    "\n",
    "  for i_state_sample, state_ids in enumerate(all_state_samples):\n",
    "    pipelines = generate_pipelines(state_ids)\n",
    "    for pipeline in pipelines:\n",
    "      result = execute_pipeline(pipeline)\n",
    "      result['state_sample'] = i_state_sample\n",
    "      results.append(result)\n",
    "      pbar.update(1)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df.to_csv(RESULTS_FILE)\n",
    "print(f'Saved scores to {RESULTS_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4121734fd62df77af0346899b5494e4291ab6203437ffd47de4eeaba662aa73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rc-plants')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
