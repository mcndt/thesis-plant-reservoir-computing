{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../'))  # for importing local packages from src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentDataset:\n",
    "    \"\"\"Wrapper class for handling a dataset from a single RC experiment run.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_df=None, csv_path=None):\n",
    "        \"\"\"Load a experiment dataset from csv file.\"\"\"\n",
    "        self._inputs: pd.DataFrame = None\n",
    "        self._outputs: pd.DataFrame = None\n",
    "        self._state: pd.DataFrame = None\n",
    "\n",
    "        self._input_keys = None\n",
    "        self._output_keys = None\n",
    "        self._state_keys = None\n",
    "\n",
    "        if dataset_df is not None:\n",
    "            self.load_data(dataset_df)\n",
    "        elif csv_path is not None:\n",
    "            self.load_dataframe(csv_path)\n",
    "        else:\n",
    "            raise Exception('Must set kwarg \"dataset_df\" or \"csv_path\"')\n",
    "\n",
    "    def load_dataframe(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.load_data(df)\n",
    "\n",
    "    def load_data(self, dataset_df):\n",
    "        self._inputs = dataset_df[dataset_df[\"type\"] == \"INPUT\"].dropna(\n",
    "            how=\"all\", axis=1\n",
    "        )\n",
    "        self._outputs = dataset_df[dataset_df[\"type\"] == \"OUTPUT\"].dropna(\n",
    "            how=\"all\", axis=1\n",
    "        )\n",
    "        self._state = dataset_df[dataset_df[\"type\"] == \"STATE\"].dropna(\n",
    "            how=\"all\", axis=1\n",
    "        )\n",
    "        self._state[\"state_id\"] = self._state[\"state_id\"].astype(int)\n",
    "        assert len(self._inputs) == len(\n",
    "            self._outputs\n",
    "        ), \"Input and output set have different lengths.\"\n",
    "\n",
    "        input_col_names = self._inputs.columns\n",
    "        self._input_keys = tuple(filter(lambda x: x.startswith(\"input_\"), input_col_names))\n",
    "        output_col_names = self._outputs.columns\n",
    "        self._output_keys = tuple(filter(lambda x: x.startswith(\"output_\"), output_col_names))\n",
    "        state_col_names = self._state.loc[\n",
    "            :, ~self._state.columns.isin([\"state_id\", \"state_type\"])\n",
    "        ].columns\n",
    "        self._state_keys = tuple(filter(lambda x: x.startswith(\"state_\"), state_col_names))\n",
    "\n",
    "        self.cache_state()\n",
    "\n",
    "    def cache_state(self) -> tuple:\n",
    "        states = self._state\n",
    "        n_runs = self.n_runs()\n",
    "        n_steps = self.n_steps()\n",
    "        state_size = self.state_size()\n",
    "        state_vars = self.get_state_variables()\n",
    "        n_vars = len(state_vars)\n",
    "\n",
    "        self._state_nd = np.empty((n_runs, n_steps, state_size, n_vars))\n",
    "        \n",
    "        node_ids = states['state_id'].unique()\n",
    "        node_map = { node_id : i for (i, node_id) in enumerate(node_ids)}\n",
    "        runs_df = states.groupby(['run_id', 'state_id'])\n",
    "        for (i_run, i_node), run_df in runs_df:\n",
    "            for i_var, var in enumerate(state_vars):\n",
    "                self._state_nd[i_run, :, node_map[i_node], i_var] = run_df.loc[:, var]\n",
    "\n",
    "    def get_input_variables(self) -> tuple:\n",
    "        \"\"\"Get the input keys available.\"\"\"\n",
    "        return self._input_keys\n",
    "\n",
    "    def get_output_variables(self) -> tuple:\n",
    "        \"\"\"Get the input output available.\"\"\"\n",
    "        return self._output_keys\n",
    "\n",
    "    def get_targets(self) -> tuple:\n",
    "        \"\"\"Get the target keys available.\"\"\"\n",
    "        return (*self.get_input_variables(), *self.get_output_variables())\n",
    "\n",
    "    def get_state_variables(self) -> tuple:\n",
    "        \"\"\"Get the state variables available.\"\"\"\n",
    "        return self._state_keys\n",
    "\n",
    "    def n_runs(self) -> int:\n",
    "        return len(self._inputs.groupby(\"run_id\"))\n",
    "\n",
    "    def n_steps(self) -> int:\n",
    "        return self._inputs.groupby(\"run_id\").size()[0]\n",
    "\n",
    "    def state_size(self) -> int:\n",
    "        return len(self._state.groupby(\"state_id\"))\n",
    "\n",
    "    def get_target(self, target_key, run_id) -> pd.Series:\n",
    "        \"Get a target signal as pandas Series by the target key.\"\n",
    "        assert (\n",
    "            target_key in self.get_targets()\n",
    "        ), f\"{target_key} not in available targets.\"\n",
    "        source = self._inputs if target_key.startswith(\"input_\") else self._outputs\n",
    "        source = source.groupby(\"run_id\").get_group(run_id)\n",
    "        target_series = source[target_key]\n",
    "        target_series.index = source[\"time\"]\n",
    "        return target_series\n",
    "\n",
    "    def get_state(self, state_key, run_id) -> pd.DataFrame:\n",
    "        \"Get the entire reservoir state of variable as pandas DataFrame by the state key.\"\n",
    "        assert (\n",
    "            state_key in self.get_state_variables()\n",
    "        ), f\"{state_key} not in available state variables.\"\n",
    "\n",
    "        i_state = self.get_state_variables().index(state_key)\n",
    "        return self._state_nd[run_id, :, :, i_state]\n",
    "        # source = self._state.groupby(\"run_id\").get_group(run_id)\n",
    "        # return source.pivot(index=\"time\", columns=[\"state_id\"], values=state_key)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"Dataset properties:\\n\"\n",
    "            f\"\\tn_runs:     {self.n_runs():>3}\\n\"\n",
    "            f\"\\tn_steps:    {self.n_steps():>3}\\n\"\n",
    "            f\"\\tstate_size: {self.state_size():>3}\\n\"\n",
    "            f\"\\nAvailable targets: \\n\\t{', '.join(self.get_targets())}\\n\"\n",
    "            f\"\\nAvailable state variables: \\n\\t{', '.join(self.get_state_variables())}\\n\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ExperimentDataset(dataset_df=dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time dataset.get_state('state_Tlc', 0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = dataset._state\n",
    "\n",
    "n_runs = dataset.n_runs()\n",
    "n_steps = dataset.n_steps()\n",
    "state_size = dataset.state_size()\n",
    "n_vars = len(dataset.get_state_variables())\n",
    "\n",
    "states_nd = np.empty((n_runs, n_steps, state_size, n_vars))\n",
    "\n",
    "node_ids = states['state_id'].unique()\n",
    "node_map = { node_id : i for (i, node_id) in enumerate(node_ids)}\n",
    "\n",
    "runs_df = states.groupby(['run_id', 'state_id'])\n",
    "state_vars = dataset.get_state_variables()\n",
    "for (i_run, i_node), run_df in runs_df:\n",
    "  for i_var, var in enumerate(state_vars):\n",
    "    states_nd[i_run, :, node_map[i_node], i_var] = run_df.loc[:, var]\n",
    "\n",
    "# states.groupby(['run_id']).apply(lambda x : x.pivot(index=\"time\", columns=[\"state_id\"], values='state_Tlc'))\n",
    "\n",
    "# for dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n",
      "(168, 360)\n"
     ]
    }
   ],
   "source": [
    "def direct_target_generator(dataset: ExperimentDataset, target: str, run_ids: [int]):\n",
    "  \"\"\"Returns a function that generates the target from the run id.\"\"\"\n",
    "  assert target in dataset.get_targets(), f\"{target} not available in dataset.\"\n",
    "\n",
    "  # preload data in numpy array for performance reasons\n",
    "  data = np.empty((len(run_ids), dataset.n_steps()))\n",
    "  for run_id in run_ids:\n",
    "    data[run_id, :] = dataset.get_target(target, run_id).to_numpy()\n",
    "\n",
    "  for run_id in run_ids:\n",
    "    yield data[run_id, :]\n",
    "\n",
    "\n",
    "def direct_reservoir_generator(dataset: ExperimentDataset, state_var: str, run_ids: [int]):\n",
    "  \"\"\"Returns a function that generates the reservoir from the run id.\"\"\"\n",
    "  assert state_var in dataset.get_state_variables(), f\"{state_var} not available in dataset.\"\n",
    "\n",
    "  for run_id in run_ids:\n",
    "    yield dataset.get_state(state_var, run_id)\n",
    "\n",
    "\n",
    "target_generator = direct_target_generator(dataset, TARGET, RUN_IDS)\n",
    "reservoir_generator = direct_reservoir_generator(dataset, STATE_VAR, RUN_IDS)\n",
    "\n",
    "print(next(target_generator).shape)\n",
    "print(next(reservoir_generator).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_random_subset(state: pd.DataFrame, state_size: int) -> pd.DataFrame:\n",
    "  choice = np.random.choice(state.shape[1], size=state_size, replace=False)\n",
    "  return state.iloc[:, choice]\n",
    "\n",
    "\n",
    "def preprocess_data(dataset, run_ids, target_generator, reservoir_generator,  \n",
    "                    state_size=32, warmup_steps=0, day_mask=None):\n",
    "  # 1. Take a random subsample of observation nodes\n",
    "  state_choice = np.random.choice(dataset.state_size(), size=state_size, replace=False)\n",
    "  \n",
    "  # 2. Cast target and reservoir state into NumPy ndarrays.\n",
    "  X = np.empty((len(run_ids), dataset.n_steps(), state_size)) # shape (runs, time_steps, nodes)\n",
    "  y = np.empty((len(run_ids), dataset.n_steps()))             # shape (runs, time_steps)\n",
    "  \n",
    "  for i_run, run_state in enumerate(reservoir_generator):\n",
    "    X[i_run, :, :] = run_state[:, state_choice]\n",
    "    \n",
    "  for i_run, run_target in enumerate(target_generator):\n",
    "    y[i_run, :] = run_target\n",
    "\n",
    "  # 3. Masks are applied.\n",
    "  if day_mask is None: \n",
    "    time_mask = np.ones(X.shape[1], dtype=bool)\n",
    "  else:\n",
    "    n_days = X.shape[1] // len(day_mask)\n",
    "    assert dataset.n_steps() % len(day_mask) == 0, \"Dataset time steps must be multiple of day mask.\"\n",
    "    time_mask = np.tile(day_mask, n_days) \n",
    "  \n",
    "  time_mask[:warmup_steps] = False\n",
    "  X = X[:, time_mask, :]\n",
    "  y = y[:, time_mask]\n",
    "\n",
    "  # 4. Normalize target and reservoir states\n",
    "  X = (X - X.mean()) / X.std()\n",
    "  y = (y - y.mean()) / y.std()\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.learning.preprocessing import generate_mask\n",
    "\n",
    "csv_path = '../datasets/hydroshoot_large_trimmed.csv'\n",
    "dataset = ExperimentDataset(csv_path=csv_path)\n",
    "\n",
    "RUN_IDS = np.arange(dataset.n_runs())\n",
    "WARMUP_STEPS = 4 * 24\n",
    "DAY_MASK = generate_mask(5, 21)\n",
    "\n",
    "STATE_SIZE = 16\n",
    "\n",
    "TARGET = 'input_Tac'\n",
    "STATE_VAR = 'state_An'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_generator = direct_target_generator(dataset, TARGET, RUN_IDS)\n",
    "reservoir_generator = direct_reservoir_generator(dataset, STATE_VAR, RUN_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 48, 16)\n",
      "(84, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\AppData\\Local\\Temp/ipykernel_31380/3507986497.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X = (X - X.mean()) / X.std()\n",
      "C:\\Users\\Max\\AppData\\Local\\Temp/ipykernel_31380/3507986497.py:35: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  y = (y - y.mean()) / y.std()\n",
      "C:\\Users\\Max\\AppData\\Local\\Temp/ipykernel_31380/3507986497.py:35: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y = (y - y.mean()) / y.std()\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X, y = preprocess_data(dataset, RUN_IDS, target_generator, reservoir_generator, state_size=STATE_SIZE,  warmup_steps=WARMUP_STEPS, day_mask=DAY_MASK)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4121734fd62df77af0346899b5494e4291ab6203437ffd47de4eeaba662aa73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rc-plants')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
