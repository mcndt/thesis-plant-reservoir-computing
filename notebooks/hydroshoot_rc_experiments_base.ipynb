{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multirun experiment pipeline 0.3.0 (CN-Wheat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))  # for importing local packages from src\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets\n",
    "\n",
    "These datasets were collected and converted in the WheatFspm repository.\n",
    "\n",
    "There are three simulations made available in the WheatFspm repository that are useable for RC experiments: NEMA H0, H3 and H15.\n",
    "\n",
    "We can try using these datasets in two different ways:\n",
    "\n",
    "1. Treat every dataset as a separate plant, training a readout for each simulation run.\n",
    "2. Concatenating the three datasets as observed behavior of a single plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.rc_dataset import ExperimentDataset\n",
    "\n",
    "\n",
    "DATASET_PATH = 'datasets/hydroshoot_large_trimmed.csv'\n",
    "\n",
    "dataset = ExperimentDataset(csv_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = 'HydroShoot_large'\n",
    "STATE_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from src.learning.scorers import nmse_scorer\n",
    "from pipeline_hydroshoot import GroupGenerator, TimeGenerator\n",
    "from pipeline_base import TrainTestSplitter, DirectTransform, WarmupTransform\n",
    "\n",
    "\n",
    "readout_model = Pipeline([\n",
    "  ('ridge_regression', Ridge(alpha=1, fit_intercept=True))\n",
    "])\n",
    "\n",
    "model_param_grid = [{\n",
    "  'ridge_regression__alpha': 10 ** np.linspace(np.log10(1e-4), np.log10(1e2), 50)\n",
    "}]\n",
    "\n",
    "datasets = (dataset,)\n",
    "run_ids = dataset.get_run_ids()\n",
    "np.random.seed(42)\n",
    "state_ids = np.random.choice(dataset.state_size(), size=16, replace=False)\n",
    "\n",
    "shared_pipeline_params = {\n",
    "  # Data generation\n",
    "  'datasets': datasets,\n",
    "  'groups': GroupGenerator(day_length=24, run_ids=run_ids, days_between_runs=1),\n",
    "  'time': TimeGenerator(day_length=24, run_ids=run_ids),\n",
    "\n",
    "  # Data transformation\n",
    "  'transforms': [\n",
    "    WarmupTransform(warmup_days=4, day_length=24),\n",
    "    DirectTransform()\n",
    "  ],\n",
    "  \n",
    "  # Model training and validation\n",
    "  'readout_model': readout_model,\n",
    "  'model_param_grid': model_param_grid,\n",
    "  'model_scorer': nmse_scorer,\n",
    "  'folds': GroupKFold(n_splits=5),\n",
    "  'train_test_split': TrainTestSplitter(block_size=4, test_ratio=0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating model fit pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config_hydroshoot import (\n",
    "  targets, \n",
    "  state_variables,\n",
    "  baseline_reservoirs,\n",
    "  heterogeneous_reservoirs, \n",
    ")\n",
    "\n",
    "target_state_pairs = [(target, state_var) for target in targets for state_var in state_variables]\n",
    "target_env_pairs = [(name, target, env_targets) for target in targets for (name, env_targets) in baseline_reservoirs]\n",
    "target_het_pairs = [(name, target, state_vars) for target in targets for (name, state_vars) in heterogeneous_reservoirs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_base import RCPipeline, Rescale, DaylightMask\n",
    "from pipeline_hydroshoot import (\n",
    "  TargetGenerator, \n",
    "  SingleReservoirGenerator,\n",
    "  MultiReservoirGenerator,\n",
    "  TargetReservoirGenerator,\n",
    "  GroupRescale\n",
    ")\n",
    "\n",
    "def generate_single(target_state_pairs):\n",
    "  for target, state_var in target_state_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': state_var, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target, run_ids=run_ids),\n",
    "      reservoir=SingleReservoirGenerator(state_var=state_var, run_ids=run_ids, state_ids=state_ids),\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=False)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def generate_multi(target_het_pairs):\n",
    "  for name, target, state_vars in target_het_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': name, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target, run_ids=run_ids),\n",
    "      reservoir=MultiReservoirGenerator(state_vars=state_vars, run_ids=run_ids, state_ids=state_ids),\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        GroupRescale(datasets=datasets, state_vars=state_vars, state_ids=state_ids)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline\n",
    "\n",
    "\n",
    "def generate_env(target_env_pairs):\n",
    "  for name, target, env_targets in target_env_pairs:\n",
    "    pipeline = RCPipeline(\n",
    "      metadata={'target_id': target, 'reservoir_id': name, 'dataset_id': DATASET_ID},\n",
    "      # Data generation\n",
    "      target=TargetGenerator(target=target, run_ids=run_ids),\n",
    "      reservoir=TargetReservoirGenerator(targets=env_targets, run_ids=run_ids),\n",
    "      # Data preprocessing\n",
    "      preprocessing=[\n",
    "        DaylightMask(day_length=24, start=5, end=21),\n",
    "        Rescale(per_feature=True)\n",
    "      ],\n",
    "      **shared_pipeline_params,\n",
    "    )\n",
    "    yield pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pipelines: 112\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "pipelines = list(chain(\n",
    "  generate_single(target_state_pairs),\n",
    "  generate_multi(target_het_pairs),\n",
    "  generate_env(target_env_pairs)\n",
    "))\n",
    "\n",
    "print(f'Total pipelines: {len(pipelines)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:34<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scores to results_hydroshoot_base_timemask_refactor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from pipeline_base import execute_pipeline\n",
    "\n",
    "\n",
    "RESULTS_FILE = 'results_hydroshoot_base_timemask_refactor.csv'\n",
    "\n",
    "total_loops = len(pipelines)\n",
    "results = []\n",
    "\n",
    "\n",
    "with tqdm(total=total_loops) as pbar:\n",
    "  for pipeline in pipelines:\n",
    "    result = execute_pipeline(pipeline)\n",
    "    results.append(result)\n",
    "    pbar.update(1)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df.to_csv(RESULTS_FILE)\n",
    "print(f'Saved scores to {RESULTS_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4121734fd62df77af0346899b5494e4291ab6203437ffd47de4eeaba662aa73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rc-plants')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
